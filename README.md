THARUN KUMAR VASPARI -CPSC-8430-DEEP-LEARNING-HW1

1-1-1.	Simulate a Function
https://github.com/VASPARITHARUNKUMAR/CPSC-8430---Deep-Learning---HW1/blob/main/HW1_SIMULATE_FUNCTION.ipynb

1-1-2.	Train on Actual Task:
https://github.com/VASPARITHARUNKUMAR/CPSC-8430---Deep-Learning---HW1/blob/main/HW1_Train_On_Actual_Task_MNIST.ipynb


1-2-1.  Visualize the Optimization Process.
https://github.com/VASPARITHARUNKUMAR/CPSC-8430---Deep-Learning---HW1/blob/main/HW1_Principal_Component.ipynb


1-2-2. Observe gradient norm during training
https://github.com/VASPARITHARUNKUMAR/CPSC-8430---Deep-Learning---HW1/blob/main/HW1_gradnorm.ipynb


1-2-3. What happens when gradient is almost zero?
https://github.com/VASPARITHARUNKUMAR/CPSC-8430---Deep-Learning---HW1/blob/main/min_grad.ipynb


1-3-1. Can network fit random labels?
https://github.com/VASPARITHARUNKUMAR/CPSC-8430---Deep-Learning---HW1/blob/main/HW1_Labelfit.ipynb


1-3-2. Number of parameters vs Generalization
https://github.com/VASPARITHARUNKUMAR/CPSC-8430---Deep-Learning---HW1/blob/main/HW1_PARAMETER_COMPARE.ipynb


1-3-3. Flatness vs Generalization
https://github.com/VASPARITHARUNKUMAR/CPSC-8430---Deep-Learning---HW1/blob/main/HW1_Interpolation.ipynb
